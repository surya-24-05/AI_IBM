{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-29T17:35:56.056060Z","iopub.execute_input":"2023-10-29T17:35:56.056605Z","iopub.status.idle":"2023-10-29T17:35:56.095968Z","shell.execute_reply.started":"2023-10-29T17:35:56.056556Z","shell.execute_reply":"2023-10-29T17:35:56.094600Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/kaggle/input/twitter-airline-sentiment/Tweets.csv\n/kaggle/input/twitter-airline-sentiment/database.sqlite\n/kaggle/input/huggingface-tf2-roberta-base-class-weights/__results__.html\n/kaggle/input/huggingface-tf2-roberta-base-class-weights/__resultx__.html\n/kaggle/input/huggingface-tf2-roberta-base-class-weights/__notebook__.ipynb\n/kaggle/input/huggingface-tf2-roberta-base-class-weights/__output__.json\n/kaggle/input/huggingface-tf2-roberta-base-class-weights/new_submission.csv\n/kaggle/input/huggingface-tf2-roberta-base-class-weights/custom.css\n/kaggle/input/roberta-base/rust_model.ot\n/kaggle/input/roberta-base/config.json\n/kaggle/input/roberta-base/merges.txt\n/kaggle/input/roberta-base/README.md\n/kaggle/input/roberta-base/tokenizer.json\n/kaggle/input/roberta-base/vocab.json\n/kaggle/input/roberta-base/tf_model.h5\n/kaggle/input/roberta-base/dict.txt\n/kaggle/input/roberta-base/pytorch_model.bin\n/kaggle/input/roberta-base/flax_model.msgpack\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:35:56.098635Z","iopub.execute_input":"2023-10-29T17:35:56.100101Z","iopub.status.idle":"2023-10-29T17:35:56.109455Z","shell.execute_reply.started":"2023-10-29T17:35:56.100017Z","shell.execute_reply":"2023-10-29T17:35:56.108079Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/twitter-airline-sentiment/Tweets.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:35:56.111439Z","iopub.execute_input":"2023-10-29T17:35:56.112218Z","iopub.status.idle":"2023-10-29T17:35:56.237075Z","shell.execute_reply.started":"2023-10-29T17:35:56.112176Z","shell.execute_reply":"2023-10-29T17:35:56.236069Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:35:56.240416Z","iopub.execute_input":"2023-10-29T17:35:56.241229Z","iopub.status.idle":"2023-10-29T17:35:56.250358Z","shell.execute_reply.started":"2023-10-29T17:35:56.241183Z","shell.execute_reply":"2023-10-29T17:35:56.248564Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(14640, 15)"},"metadata":{}}]},{"cell_type":"markdown","source":"The data size is 14640 x 15","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:35:56.252074Z","iopub.execute_input":"2023-10-29T17:35:56.252584Z","iopub.status.idle":"2023-10-29T17:35:56.283547Z","shell.execute_reply.started":"2023-10-29T17:35:56.252550Z","shell.execute_reply":"2023-10-29T17:35:56.282227Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tweet_id                            0\nairline_sentiment                   0\nairline_sentiment_confidence        0\nnegativereason                   5462\nnegativereason_confidence        4118\nairline                             0\nairline_sentiment_gold          14600\nname                                0\nnegativereason_gold             14608\nretweet_count                       0\ntext                                0\ntweet_coord                     13621\ntweet_created                       0\ntweet_location                   4733\nuser_timezone                    4820\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"data[\"airline\"].drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:35:56.285384Z","iopub.execute_input":"2023-10-29T17:35:56.285782Z","iopub.status.idle":"2023-10-29T17:35:56.305106Z","shell.execute_reply.started":"2023-10-29T17:35:56.285749Z","shell.execute_reply":"2023-10-29T17:35:56.304018Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0        Virgin America\n504              United\n4326          Southwest\n6746              Delta\n8966         US Airways\n11879          American\nName: airline, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"data[\"airline_sentiment\"].drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:35:56.307370Z","iopub.execute_input":"2023-10-29T17:35:56.308322Z","iopub.status.idle":"2023-10-29T17:35:56.322548Z","shell.execute_reply.started":"2023-10-29T17:35:56.308278Z","shell.execute_reply":"2023-10-29T17:35:56.320832Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0     neutral\n1    positive\n3    negative\nName: airline_sentiment, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"PHASE 2 BEGINS","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:35:56.324225Z","iopub.execute_input":"2023-10-29T17:35:56.324653Z","iopub.status.idle":"2023-10-29T17:35:56.770167Z","shell.execute_reply.started":"2023-10-29T17:35:56.324619Z","shell.execute_reply":"2023-10-29T17:35:56.769197Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**PHASE 3- DEVELOPMENT part 1**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the dataset\ndataset_path = '/kaggle/input/twitter-airline-sentiment/Tweets.csv'\ndf = pd.read_csv(dataset_path)\n\n# Select relevant columns\ndf = df[['text', 'airline_sentiment']]\n\n# Split the dataset into training, validation, and test sets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n\n# Text preprocessing\n# You can customize this part to suit your specific data cleaning needs.\n# Common preprocessing steps include lowercasing, removing special characters, and tokenization.\n\n# Vectorize the text data using Count Vectorization and TF-IDF transformation\ncount_vectorizer = CountVectorizer()\ntfidf_transformer = TfidfTransformer()\n\nX_train_counts = count_vectorizer.fit_transform(train_df['text'])\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n\nX_val_counts = count_vectorizer.transform(val_df['text'])\nX_val_tfidf = tfidf_transformer.transform(X_val_counts)\n\nX_test_counts = count_vectorizer.transform(test_df['text'])\nX_test_tfidf = tfidf_transformer.transform(X_test_counts)\n\n# Encode the sentiment labels\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(train_df['airline_sentiment'])\ny_val = label_encoder.transform(val_df['airline_sentiment'])\ny_test = label_encoder.transform(test_df['airline_sentiment'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:35:56.771825Z","iopub.execute_input":"2023-10-29T17:35:56.773005Z","iopub.status.idle":"2023-10-29T17:35:57.376977Z","shell.execute_reply.started":"2023-10-29T17:35:56.772958Z","shell.execute_reply":"2023-10-29T17:35:57.375561Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:35:57.381425Z","iopub.execute_input":"2023-10-29T17:35:57.381832Z","iopub.status.idle":"2023-10-29T17:36:11.787015Z","shell.execute_reply.started":"2023-10-29T17:35:57.381802Z","shell.execute_reply":"2023-10-29T17:36:11.785478Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**PHASE 4- DEVELOPMENT part 2**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\nfrom tqdm import tqdm\n\n# Load and preprocess the dataset (as mentioned in your previous code)\n\n# Initialize RoBERTa tokenizer and model from your locally downloaded 'roberta-base'\ntokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/input/roberta-base\")\nmodel = RobertaForSequenceClassification.from_pretrained(\"/kaggle/input/roberta-base\", num_labels=3)  # 3 labels: positive, negative, neutral\n\n# Tokenize and encode the text data\nX_train_encoded = tokenizer(train_df['text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\nX_val_encoded = tokenizer(val_df['text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n\n# Convert labels to PyTorch tensors\ny_train_tensor = torch.tensor(y_train)\ny_val_tensor = torch.tensor(y_val)\n\n# Create data loaders\ntrain_data = TensorDataset(X_train_encoded.input_ids, X_train_encoded.attention_mask, y_train_tensor)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n\nval_data = TensorDataset(X_val_encoded.input_ids, X_val_encoded.attention_mask, y_val_tensor)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=64)\n\n# Set up training parameters\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 2)\n\n# Training loop (as in the previous code)\n\n# Now, you can use the trained model for sentiment analysis\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:36:11.789553Z","iopub.execute_input":"2023-10-29T17:36:11.790161Z","iopub.status.idle":"2023-10-29T17:36:21.097404Z","shell.execute_reply.started":"2023-10-29T17:36:11.790100Z","shell.execute_reply":"2023-10-29T17:36:21.095680Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaForSequenceClassification\n\n# Specify the path to your local directory containing the \"roberta-base\" model\nlocal_model_path = \"/kaggle/input/roberta-base\"\n\n# Load the RoBERTa model and tokenizer from the local directory\nmodel = RobertaForSequenceClassification.from_pretrained(local_model_path, num_labels=3)\ntokenizer = RobertaTokenizer.from_pretrained(local_model_path)\n\n# Rest of your code for testing the model remains the same\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:36:21.099276Z","iopub.execute_input":"2023-10-29T17:36:21.099788Z","iopub.status.idle":"2023-10-29T17:36:22.827869Z","shell.execute_reply.started":"2023-10-29T17:36:21.099742Z","shell.execute_reply":"2023-10-29T17:36:22.826574Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\n\n# Load the pre-trained RoBERTa model and tokenizer\nmodel = RobertaForSequenceClassification.from_pretrained(\"/kaggle/input/roberta-base\", num_labels=3)\ntokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/input/roberta-base\")\n\n# Define the tweet\ntweet = \"such an agonizing experience caused by these extraordinary so-called airlines\"\n\n# Tokenize and encode the tweet\nencoded_tweet = tokenizer(tweet, padding=True, truncation=True, return_tensors=\"pt\")\n\n# Make predictions\nwith torch.no_grad():\n    model.eval()\n    outputs = model(**encoded_tweet)\n    predictions = torch.argmax(outputs.logits, dim=1)\n\n# Map predictions to labels\nsentiment_labels = ['negative', 'neutral', 'positive']\npredicted_sentiment = sentiment_labels[predictions.item()]\n\nprint(\"The sentiment of the tweet is: \",{predicted_sentiment})\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:36:22.829484Z","iopub.execute_input":"2023-10-29T17:36:22.830274Z","iopub.status.idle":"2023-10-29T17:36:25.090536Z","shell.execute_reply.started":"2023-10-29T17:36:22.830239Z","shell.execute_reply":"2023-10-29T17:36:25.089369Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"The sentiment of the tweet is:  {'neutral'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**PHASE 5 - Fine tuning our ML model**","metadata":{}},{"cell_type":"code","source":"pip install transformers\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:36:25.092375Z","iopub.execute_input":"2023-10-29T17:36:25.092941Z","iopub.status.idle":"2023-10-29T17:36:39.172938Z","shell.execute_reply.started":"2023-10-29T17:36:25.092895Z","shell.execute_reply":"2023-10-29T17:36:39.171295Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/input/roberta-base\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:36:39.175345Z","iopub.execute_input":"2023-10-29T17:36:39.175869Z","iopub.status.idle":"2023-10-29T17:36:39.314852Z","shell.execute_reply.started":"2023-10-29T17:36:39.175824Z","shell.execute_reply":"2023-10-29T17:36:39.313102Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"pip install wandb","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:36:39.316938Z","iopub.execute_input":"2023-10-29T17:36:39.317830Z","iopub.status.idle":"2023-10-29T17:36:53.625808Z","shell.execute_reply.started":"2023-10-29T17:36:39.317767Z","shell.execute_reply":"2023-10-29T17:36:53.624075Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.12)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.30.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.0.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install wandb --upgrade","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:36:53.627830Z","iopub.execute_input":"2023-10-29T17:36:53.628275Z","iopub.status.idle":"2023-10-29T17:37:08.252879Z","shell.execute_reply.started":"2023-10-29T17:36:53.628242Z","shell.execute_reply":"2023-10-29T17:37:08.251105Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.12)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.30.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.0.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer, TrainingArguments, Trainer\nfrom sklearn.model_selection import train_test_split\n\n# Load the pre-trained RoBERTa model and tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\nmodel = RobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n\n# Load your dataset from a CSV file\ndf = pd.read_csv('/kaggle/input/twitter-airline-sentiment/Tweets.csv')\n\n\n# Split the dataset into train and validation sets\ntrain_df, validation_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Tokenize and preprocess the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True)\n\ntrain_dataset = tokenize_function(train_df.to_dict(orient='list'))\nvalidation_dataset = tokenize_function(validation_df.to_dict(orient='list'))\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    save_steps=500,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    save_total_limit=2,\n    remove_unused_columns=False,\n    push_to_hub=False,\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    tokenizer=tokenizer,\n)\n\n# Start fine-tuning\ntrainer.train()\n\n# Save the model\ntrainer.save_model('AUC_roberta_base')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:42:07.039185Z","iopub.execute_input":"2023-10-29T17:42:07.039665Z","iopub.status.idle":"2023-10-29T17:42:17.297329Z","shell.execute_reply.started":"2023-10-29T17:42:07.039630Z","shell.execute_reply":"2023-10-29T17:42:17.295090Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 45\u001b[0m\n\u001b[1;32m     36\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     37\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     38\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Start fine-tuning\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m     48\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUC_roberta_base\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1813\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1810\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1813\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1814\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:384\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:247\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key][item] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key. Only three types of key are available: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m     )\n","\u001b[0;31mKeyError\u001b[0m: 'Invalid key. Only three types of key are available: (1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.'"],"ename":"KeyError","evalue":"'Invalid key. Only three types of key are available: (1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.'","output_type":"error"}]},{"cell_type":"code","source":"from transformers import RobertaForSequenceClassification, RobertaTokenizer\n\nmodel = RobertaForSequenceClassification.from_pretrained(\"roberta-base\")\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:43:38.563558Z","iopub.execute_input":"2023-10-29T17:43:38.564042Z","iopub.status.idle":"2023-10-29T17:43:40.596028Z","shell.execute_reply.started":"2023-10-29T17:43:38.564007Z","shell.execute_reply":"2023-10-29T17:43:40.593795Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaForSequenceClassification\nimport torch\n\n# Load the fine-tuned model and tokenizer\nmodel = RobertaForSequenceClassification.from_pretrained(\"/kaggle/input/roberta-base\")\ntokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/input/roberta-base\")\n\n# Define the custom tweet you want to test\ncustom_tweet = \"This airline need not exist\"\n\n# Tokenize the custom tweet\ninputs = tokenizer(custom_tweet, return_tensors=\"pt\", padding=True, truncation=True)\n\n# Make predictions\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits = outputs.logits\n\n# Assuming your model is for binary classification (e.g., sentiment analysis)\nprobabilities = torch.softmax(logits, dim=1)\npredicted_class = torch.argmax(probabilities, dim=1).item()\n\n# Define the class labels for your task\nclass_labels = [\"Negative\", \"Positive\"]\n\n# Get the predicted sentiment\npredicted_sentiment = class_labels[predicted_class]\n\nprint(f\"Custom Tweet: {custom_tweet}\")\nprint(f\"Predicted Sentiment: {predicted_sentiment}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T18:17:57.002138Z","iopub.execute_input":"2023-10-29T18:17:57.002595Z","iopub.status.idle":"2023-10-29T18:17:58.661469Z","shell.execute_reply.started":"2023-10-29T18:17:57.002562Z","shell.execute_reply":"2023-10-29T18:17:58.658273Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"Custom Tweet: This airline need not exist\nPredicted Sentiment: Negative\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
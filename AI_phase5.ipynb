{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-29T16:26:23.856727Z","iopub.execute_input":"2023-10-29T16:26:23.857267Z","iopub.status.idle":"2023-10-29T16:26:23.877770Z","shell.execute_reply.started":"2023-10-29T16:26:23.857220Z","shell.execute_reply":"2023-10-29T16:26:23.876809Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/input/huggingface-tf2-roberta-base-class-weights/__results__.html\n/kaggle/input/huggingface-tf2-roberta-base-class-weights/__resultx__.html\n/kaggle/input/huggingface-tf2-roberta-base-class-weights/__notebook__.ipynb\n/kaggle/input/huggingface-tf2-roberta-base-class-weights/__output__.json\n/kaggle/input/huggingface-tf2-roberta-base-class-weights/new_submission.csv\n/kaggle/input/huggingface-tf2-roberta-base-class-weights/custom.css\n/kaggle/input/roberta-base/rust_model.ot\n/kaggle/input/roberta-base/config.json\n/kaggle/input/roberta-base/merges.txt\n/kaggle/input/roberta-base/README.md\n/kaggle/input/roberta-base/tokenizer.json\n/kaggle/input/roberta-base/vocab.json\n/kaggle/input/roberta-base/tf_model.h5\n/kaggle/input/roberta-base/dict.txt\n/kaggle/input/roberta-base/pytorch_model.bin\n/kaggle/input/roberta-base/flax_model.msgpack\n/kaggle/input/twitter-airline-sentiment/Tweets.csv\n/kaggle/input/twitter-airline-sentiment/database.sqlite\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:23.879809Z","iopub.execute_input":"2023-10-29T16:26:23.880742Z","iopub.status.idle":"2023-10-29T16:26:23.885373Z","shell.execute_reply.started":"2023-10-29T16:26:23.880677Z","shell.execute_reply":"2023-10-29T16:26:23.884394Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/twitter-airline-sentiment/Tweets.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:23.886871Z","iopub.execute_input":"2023-10-29T16:26:23.887843Z","iopub.status.idle":"2023-10-29T16:26:23.999266Z","shell.execute_reply.started":"2023-10-29T16:26:23.887791Z","shell.execute_reply":"2023-10-29T16:26:23.998177Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:24.038849Z","iopub.execute_input":"2023-10-29T16:26:24.039303Z","iopub.status.idle":"2023-10-29T16:26:24.048114Z","shell.execute_reply.started":"2023-10-29T16:26:24.039268Z","shell.execute_reply":"2023-10-29T16:26:24.046776Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(14640, 15)"},"metadata":{}}]},{"cell_type":"markdown","source":"The data size is 14640 x 15","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:24.049338Z","iopub.execute_input":"2023-10-29T16:26:24.049726Z","iopub.status.idle":"2023-10-29T16:26:24.078858Z","shell.execute_reply.started":"2023-10-29T16:26:24.049675Z","shell.execute_reply":"2023-10-29T16:26:24.077636Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"tweet_id                            0\nairline_sentiment                   0\nairline_sentiment_confidence        0\nnegativereason                   5462\nnegativereason_confidence        4118\nairline                             0\nairline_sentiment_gold          14600\nname                                0\nnegativereason_gold             14608\nretweet_count                       0\ntext                                0\ntweet_coord                     13621\ntweet_created                       0\ntweet_location                   4733\nuser_timezone                    4820\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"data[\"airline\"].drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:24.080784Z","iopub.execute_input":"2023-10-29T16:26:24.081514Z","iopub.status.idle":"2023-10-29T16:26:24.094626Z","shell.execute_reply.started":"2023-10-29T16:26:24.081471Z","shell.execute_reply":"2023-10-29T16:26:24.092994Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"0        Virgin America\n504              United\n4326          Southwest\n6746              Delta\n8966         US Airways\n11879          American\nName: airline, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"data[\"airline_sentiment\"].drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:24.112570Z","iopub.execute_input":"2023-10-29T16:26:24.113287Z","iopub.status.idle":"2023-10-29T16:26:24.123854Z","shell.execute_reply.started":"2023-10-29T16:26:24.113253Z","shell.execute_reply":"2023-10-29T16:26:24.122361Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0     neutral\n1    positive\n3    negative\nName: airline_sentiment, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"PHASE 2 BEGINS","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:24.125412Z","iopub.execute_input":"2023-10-29T16:26:24.125952Z","iopub.status.idle":"2023-10-29T16:26:24.615289Z","shell.execute_reply.started":"2023-10-29T16:26:24.125908Z","shell.execute_reply":"2023-10-29T16:26:24.614021Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**PHASE 3- DEVELOPMENT part 1**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the dataset\ndataset_path = '/kaggle/input/twitter-airline-sentiment/Tweets.csv'\ndf = pd.read_csv(dataset_path)\n\n# Select relevant columns\ndf = df[['text', 'airline_sentiment']]\n\n# Split the dataset into training, validation, and test sets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n\n# Text preprocessing\n# You can customize this part to suit your specific data cleaning needs.\n# Common preprocessing steps include lowercasing, removing special characters, and tokenization.\n\n# Vectorize the text data using Count Vectorization and TF-IDF transformation\ncount_vectorizer = CountVectorizer()\ntfidf_transformer = TfidfTransformer()\n\nX_train_counts = count_vectorizer.fit_transform(train_df['text'])\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n\nX_val_counts = count_vectorizer.transform(val_df['text'])\nX_val_tfidf = tfidf_transformer.transform(X_val_counts)\n\nX_test_counts = count_vectorizer.transform(test_df['text'])\nX_test_tfidf = tfidf_transformer.transform(X_test_counts)\n\n# Encode the sentiment labels\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(train_df['airline_sentiment'])\ny_val = label_encoder.transform(val_df['airline_sentiment'])\ny_test = label_encoder.transform(test_df['airline_sentiment'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:24.618790Z","iopub.execute_input":"2023-10-29T16:26:24.619530Z","iopub.status.idle":"2023-10-29T16:26:25.207461Z","shell.execute_reply.started":"2023-10-29T16:26:24.619493Z","shell.execute_reply":"2023-10-29T16:26:25.206181Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:25.209150Z","iopub.execute_input":"2023-10-29T16:26:25.209638Z","iopub.status.idle":"2023-10-29T16:27:00.460327Z","shell.execute_reply.started":"2023-10-29T16:26:25.209592Z","shell.execute_reply":"2023-10-29T16:27:00.458670Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**PHASE 4- DEVELOPMENT part 2**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\nfrom tqdm import tqdm\n\n# Load and preprocess the dataset (as mentioned in your previous code)\n\n# Initialize RoBERTa tokenizer and model from your locally downloaded 'roberta-base'\ntokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/input/roberta-base\")\nmodel = RobertaForSequenceClassification.from_pretrained(\"/kaggle/input/roberta-base\", num_labels=3)  # 3 labels: positive, negative, neutral\n\n# Tokenize and encode the text data\nX_train_encoded = tokenizer(train_df['text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\nX_val_encoded = tokenizer(val_df['text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n\n# Convert labels to PyTorch tensors\ny_train_tensor = torch.tensor(y_train)\ny_val_tensor = torch.tensor(y_val)\n\n# Create data loaders\ntrain_data = TensorDataset(X_train_encoded.input_ids, X_train_encoded.attention_mask, y_train_tensor)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n\nval_data = TensorDataset(X_val_encoded.input_ids, X_val_encoded.attention_mask, y_val_tensor)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=64)\n\n# Set up training parameters\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 2)\n\n# Training loop (as in the previous code)\n\n# Now, you can use the trained model for sentiment analysis\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:27:00.462278Z","iopub.execute_input":"2023-10-29T16:27:00.462687Z","iopub.status.idle":"2023-10-29T16:27:09.120991Z","shell.execute_reply.started":"2023-10-29T16:27:00.462649Z","shell.execute_reply":"2023-10-29T16:27:09.119744Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaForSequenceClassification\n\n# Specify the path to your local directory containing the \"roberta-base\" model\nlocal_model_path = \"/kaggle/input/roberta-base\"\n\n# Load the RoBERTa model and tokenizer from the local directory\nmodel = RobertaForSequenceClassification.from_pretrained(local_model_path, num_labels=3)\ntokenizer = RobertaTokenizer.from_pretrained(local_model_path)\n\n# Rest of your code for testing the model remains the same\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:27:09.126208Z","iopub.execute_input":"2023-10-29T16:27:09.126672Z","iopub.status.idle":"2023-10-29T16:27:10.813060Z","shell.execute_reply.started":"2023-10-29T16:27:09.126638Z","shell.execute_reply":"2023-10-29T16:27:10.811914Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\n\n# Load the pre-trained RoBERTa model and tokenizer\nmodel = RobertaForSequenceClassification.from_pretrained(\"/kaggle/input/roberta-base\", num_labels=3)\ntokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/input/roberta-base\")\n\n# Define the tweet\ntweet = \"such an agonizing experience caused by these extraordinary so-called airlines\"\n\n# Tokenize and encode the tweet\nencoded_tweet = tokenizer(tweet, padding=True, truncation=True, return_tensors=\"pt\")\n\n# Make predictions\nwith torch.no_grad():\n    model.eval()\n    outputs = model(**encoded_tweet)\n    predictions = torch.argmax(outputs.logits, dim=1)\n\n# Map predictions to labels\nsentiment_labels = ['negative', 'neutral', 'positive']\npredicted_sentiment = sentiment_labels[predictions.item()]\n\nprint(\"The sentiment of the tweet is: \",{predicted_sentiment})\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:27:10.814994Z","iopub.execute_input":"2023-10-29T16:27:10.816038Z","iopub.status.idle":"2023-10-29T16:27:12.755735Z","shell.execute_reply.started":"2023-10-29T16:27:10.815992Z","shell.execute_reply":"2023-10-29T16:27:12.754531Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"The sentiment of the tweet is:  {'negative'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**PHASE 5 - Fine tuning our ML model**","metadata":{}},{"cell_type":"code","source":"pip install transformers\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:51:51.709310Z","iopub.execute_input":"2023-10-29T16:51:51.710439Z","iopub.status.idle":"2023-10-29T16:52:25.750638Z","shell.execute_reply.started":"2023-10-29T16:51:51.710400Z","shell.execute_reply":"2023-10-29T16:52:25.749249Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/input/roberta-base\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:52:50.579554Z","iopub.execute_input":"2023-10-29T16:52:50.580811Z","iopub.status.idle":"2023-10-29T16:52:50.690239Z","shell.execute_reply.started":"2023-10-29T16:52:50.580768Z","shell.execute_reply":"2023-10-29T16:52:50.689027Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer, TrainingArguments, Trainer\nfrom sklearn.model_selection import train_test_split\n\n# Load the pre-trained RoBERTa model and tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/input/roberta-base\")\nmodel = RobertaForSequenceClassification.from_pretrained(\"/kaggle/input/roberta-base\")\n\n# Load your dataset from a CSV file\ndf = pd.read_csv('/kaggle/input/twitter-airline-sentiment/Tweets.csv')\n\n# Split the dataset into train and validation sets\ntrain_df, validation_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Tokenize and preprocess the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True)\n\ntrain_dataset = tokenize_function(train_df.to_dict(orient='list'))\nvalidation_dataset = tokenize_function(validation_df.to_dict(orient='list'))\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    save_steps=500,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    save_total_limit=2,\n    remove_unused_columns=False,\n    push_to_hub=False,\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    tokenizer=tokenizer,\n)\n\n# Start fine-tuning\ntrainer.train()\n\n# Save the model\ntrainer.save_model('AUC_roberta_base')\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T17:05:13.223344Z","iopub.execute_input":"2023-10-29T17:05:13.223915Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nAsking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"}]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-29T16:26:23.856727Z","iopub.execute_input":"2023-10-29T16:26:23.857267Z","iopub.status.idle":"2023-10-29T16:26:23.877770Z","shell.execute_reply.started":"2023-10-29T16:26:23.857220Z","shell.execute_reply":"2023-10-29T16:26:23.876809Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/input/huggingface-tf2-roberta-base-class-weights/__results__.html\n/kaggle/input/huggingface-tf2-roberta-base-class-weights/__resultx__.html\n/kaggle/input/huggingface-tf2-roberta-base-class-weights/__notebook__.ipynb\n/kaggle/input/huggingface-tf2-roberta-base-class-weights/__output__.json\n/kaggle/input/huggingface-tf2-roberta-base-class-weights/new_submission.csv\n/kaggle/input/huggingface-tf2-roberta-base-class-weights/custom.css\n/kaggle/input/roberta-base/rust_model.ot\n/kaggle/input/roberta-base/config.json\n/kaggle/input/roberta-base/merges.txt\n/kaggle/input/roberta-base/README.md\n/kaggle/input/roberta-base/tokenizer.json\n/kaggle/input/roberta-base/vocab.json\n/kaggle/input/roberta-base/tf_model.h5\n/kaggle/input/roberta-base/dict.txt\n/kaggle/input/roberta-base/pytorch_model.bin\n/kaggle/input/roberta-base/flax_model.msgpack\n/kaggle/input/twitter-airline-sentiment/Tweets.csv\n/kaggle/input/twitter-airline-sentiment/database.sqlite\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:23.879809Z","iopub.execute_input":"2023-10-29T16:26:23.880742Z","iopub.status.idle":"2023-10-29T16:26:23.885373Z","shell.execute_reply.started":"2023-10-29T16:26:23.880677Z","shell.execute_reply":"2023-10-29T16:26:23.884394Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"The pandas package is imported using the variable pd","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/twitter-airline-sentiment/Tweets.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:23.886871Z","iopub.execute_input":"2023-10-29T16:26:23.887843Z","iopub.status.idle":"2023-10-29T16:26:23.999266Z","shell.execute_reply.started":"2023-10-29T16:26:23.887791Z","shell.execute_reply":"2023-10-29T16:26:23.998177Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(data)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:24.000843Z","iopub.execute_input":"2023-10-29T16:26:24.001509Z","iopub.status.idle":"2023-10-29T16:26:24.035474Z","shell.execute_reply.started":"2023-10-29T16:26:24.001474Z","shell.execute_reply":"2023-10-29T16:26:24.034041Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n0      570306133677760513           neutral                        1.0000   \n1      570301130888122368          positive                        0.3486   \n2      570301083672813571           neutral                        0.6837   \n3      570301031407624196          negative                        1.0000   \n4      570300817074462722          negative                        1.0000   \n...                   ...               ...                           ...   \n14635  569587686496825344          positive                        0.3487   \n14636  569587371693355008          negative                        1.0000   \n14637  569587242672398336           neutral                        1.0000   \n14638  569587188687634433          negative                        1.0000   \n14639  569587140490866689           neutral                        0.6771   \n\n               negativereason  negativereason_confidence         airline  \\\n0                         NaN                        NaN  Virgin America   \n1                         NaN                     0.0000  Virgin America   \n2                         NaN                        NaN  Virgin America   \n3                  Bad Flight                     0.7033  Virgin America   \n4                  Can't Tell                     1.0000  Virgin America   \n...                       ...                        ...             ...   \n14635                     NaN                     0.0000        American   \n14636  Customer Service Issue                     1.0000        American   \n14637                     NaN                        NaN        American   \n14638  Customer Service Issue                     0.6659        American   \n14639                     NaN                     0.0000        American   \n\n      airline_sentiment_gold             name negativereason_gold  \\\n0                        NaN          cairdin                 NaN   \n1                        NaN         jnardino                 NaN   \n2                        NaN       yvonnalynn                 NaN   \n3                        NaN         jnardino                 NaN   \n4                        NaN         jnardino                 NaN   \n...                      ...              ...                 ...   \n14635                    NaN  KristenReenders                 NaN   \n14636                    NaN         itsropes                 NaN   \n14637                    NaN         sanyabun                 NaN   \n14638                    NaN       SraJackson                 NaN   \n14639                    NaN        daviddtwu                 NaN   \n\n       retweet_count                                               text  \\\n0                  0                @VirginAmerica What @dhepburn said.   \n1                  0  @VirginAmerica plus you've added commercials t...   \n2                  0  @VirginAmerica I didn't today... Must mean I n...   \n3                  0  @VirginAmerica it's really aggressive to blast...   \n4                  0  @VirginAmerica and it's a really big bad thing...   \n...              ...                                                ...   \n14635              0  @AmericanAir thank you we got on a different f...   \n14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n14637              0  @AmericanAir Please bring American Airlines to...   \n14638              0  @AmericanAir you have my money, you change my ...   \n14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n\n      tweet_coord              tweet_created tweet_location  \\\n0             NaN  2015-02-24 11:35:52 -0800            NaN   \n1             NaN  2015-02-24 11:15:59 -0800            NaN   \n2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n3             NaN  2015-02-24 11:15:36 -0800            NaN   \n4             NaN  2015-02-24 11:14:45 -0800            NaN   \n...           ...                        ...            ...   \n14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n\n                    user_timezone  \n0      Eastern Time (US & Canada)  \n1      Pacific Time (US & Canada)  \n2      Central Time (US & Canada)  \n3      Pacific Time (US & Canada)  \n4      Pacific Time (US & Canada)  \n...                           ...  \n14635                         NaN  \n14636                         NaN  \n14637                         NaN  \n14638  Eastern Time (US & Canada)  \n14639                         NaN  \n\n[14640 rows x 15 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:24.038849Z","iopub.execute_input":"2023-10-29T16:26:24.039303Z","iopub.status.idle":"2023-10-29T16:26:24.048114Z","shell.execute_reply.started":"2023-10-29T16:26:24.039268Z","shell.execute_reply":"2023-10-29T16:26:24.046776Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(14640, 15)"},"metadata":{}}]},{"cell_type":"markdown","source":"The data size is 14640 x 15","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:24.049338Z","iopub.execute_input":"2023-10-29T16:26:24.049726Z","iopub.status.idle":"2023-10-29T16:26:24.078858Z","shell.execute_reply.started":"2023-10-29T16:26:24.049675Z","shell.execute_reply":"2023-10-29T16:26:24.077636Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"tweet_id                            0\nairline_sentiment                   0\nairline_sentiment_confidence        0\nnegativereason                   5462\nnegativereason_confidence        4118\nairline                             0\nairline_sentiment_gold          14600\nname                                0\nnegativereason_gold             14608\nretweet_count                       0\ntext                                0\ntweet_coord                     13621\ntweet_created                       0\ntweet_location                   4733\nuser_timezone                    4820\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"We now identify the columns that have blank responses.","metadata":{}},{"cell_type":"code","source":"data[\"airline\"].drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:24.080784Z","iopub.execute_input":"2023-10-29T16:26:24.081514Z","iopub.status.idle":"2023-10-29T16:26:24.094626Z","shell.execute_reply.started":"2023-10-29T16:26:24.081471Z","shell.execute_reply":"2023-10-29T16:26:24.092994Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"0        Virgin America\n504              United\n4326          Southwest\n6746              Delta\n8966         US Airways\n11879          American\nName: airline, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"VIRGIN AMERICA, UNITED, SOUTHWEST, DELTA, US AIRWAYS, AMERICAN are the airlines.","metadata":{}},{"cell_type":"code","source":"data[\"airline_sentiment_confidence\"].drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:24.096159Z","iopub.execute_input":"2023-10-29T16:26:24.096533Z","iopub.status.idle":"2023-10-29T16:26:24.110899Z","shell.execute_reply.started":"2023-10-29T16:26:24.096499Z","shell.execute_reply":"2023-10-29T16:26:24.109776Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0        1.0000\n1        0.3486\n2        0.6837\n6        0.6745\n7        0.6340\n          ...  \n14562    0.7257\n14569    0.7241\n14587    0.6384\n14594    0.7094\n14635    0.3487\nName: airline_sentiment_confidence, Length: 1023, dtype: float64"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"data[\"airline_sentiment\"].drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:24.112570Z","iopub.execute_input":"2023-10-29T16:26:24.113287Z","iopub.status.idle":"2023-10-29T16:26:24.123854Z","shell.execute_reply.started":"2023-10-29T16:26:24.113253Z","shell.execute_reply":"2023-10-29T16:26:24.122361Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0     neutral\n1    positive\n3    negative\nName: airline_sentiment, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"PHASE 2 BEGINS","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:24.125412Z","iopub.execute_input":"2023-10-29T16:26:24.125952Z","iopub.status.idle":"2023-10-29T16:26:24.615289Z","shell.execute_reply.started":"2023-10-29T16:26:24.125908Z","shell.execute_reply":"2023-10-29T16:26:24.614021Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**PHASE 3,4 - DEVELOPMENT**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the dataset\ndataset_path = '/kaggle/input/twitter-airline-sentiment/Tweets.csv'\ndf = pd.read_csv(dataset_path)\n\n# Select relevant columns\ndf = df[['text', 'airline_sentiment']]\n\n# Split the dataset into training, validation, and test sets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n\n# Text preprocessing\n# You can customize this part to suit your specific data cleaning needs.\n# Common preprocessing steps include lowercasing, removing special characters, and tokenization.\n\n# Vectorize the text data using Count Vectorization and TF-IDF transformation\ncount_vectorizer = CountVectorizer()\ntfidf_transformer = TfidfTransformer()\n\nX_train_counts = count_vectorizer.fit_transform(train_df['text'])\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n\nX_val_counts = count_vectorizer.transform(val_df['text'])\nX_val_tfidf = tfidf_transformer.transform(X_val_counts)\n\nX_test_counts = count_vectorizer.transform(test_df['text'])\nX_test_tfidf = tfidf_transformer.transform(X_test_counts)\n\n# Encode the sentiment labels\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(train_df['airline_sentiment'])\ny_val = label_encoder.transform(val_df['airline_sentiment'])\ny_test = label_encoder.transform(test_df['airline_sentiment'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:24.618790Z","iopub.execute_input":"2023-10-29T16:26:24.619530Z","iopub.status.idle":"2023-10-29T16:26:25.207461Z","shell.execute_reply.started":"2023-10-29T16:26:24.619493Z","shell.execute_reply":"2023-10-29T16:26:25.206181Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:26:25.209150Z","iopub.execute_input":"2023-10-29T16:26:25.209638Z","iopub.status.idle":"2023-10-29T16:27:00.460327Z","shell.execute_reply.started":"2023-10-29T16:26:25.209592Z","shell.execute_reply":"2023-10-29T16:27:00.458670Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\nfrom tqdm import tqdm\n\n# Load and preprocess the dataset (as mentioned in your previous code)\n\n# Initialize RoBERTa tokenizer and model from your locally downloaded 'roberta-base'\ntokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/input/roberta-base\")\nmodel = RobertaForSequenceClassification.from_pretrained(\"/kaggle/input/roberta-base\", num_labels=3)  # 3 labels: positive, negative, neutral\n\n# Tokenize and encode the text data\nX_train_encoded = tokenizer(train_df['text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\nX_val_encoded = tokenizer(val_df['text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n\n# Convert labels to PyTorch tensors\ny_train_tensor = torch.tensor(y_train)\ny_val_tensor = torch.tensor(y_val)\n\n# Create data loaders\ntrain_data = TensorDataset(X_train_encoded.input_ids, X_train_encoded.attention_mask, y_train_tensor)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n\nval_data = TensorDataset(X_val_encoded.input_ids, X_val_encoded.attention_mask, y_val_tensor)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=64)\n\n# Set up training parameters\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 2)\n\n# Training loop (as in the previous code)\n\n# Now, you can use the trained model for sentiment analysis\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:27:00.462278Z","iopub.execute_input":"2023-10-29T16:27:00.462687Z","iopub.status.idle":"2023-10-29T16:27:09.120991Z","shell.execute_reply.started":"2023-10-29T16:27:00.462649Z","shell.execute_reply":"2023-10-29T16:27:09.119744Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaForSequenceClassification\n\n# Specify the path to your local directory containing the \"roberta-base\" model\nlocal_model_path = \"/kaggle/input/roberta-base\"\n\n# Load the RoBERTa model and tokenizer from the local directory\nmodel = RobertaForSequenceClassification.from_pretrained(local_model_path, num_labels=3)\ntokenizer = RobertaTokenizer.from_pretrained(local_model_path)\n\n# Rest of your code for testing the model remains the same\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:27:09.126208Z","iopub.execute_input":"2023-10-29T16:27:09.126672Z","iopub.status.idle":"2023-10-29T16:27:10.813060Z","shell.execute_reply.started":"2023-10-29T16:27:09.126638Z","shell.execute_reply":"2023-10-29T16:27:10.811914Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\n\n# Load the pre-trained RoBERTa model and tokenizer\nmodel = RobertaForSequenceClassification.from_pretrained(\"/kaggle/input/roberta-base\", num_labels=3)\ntokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/input/roberta-base\")\n\n# Define the tweet\ntweet = \"such an agonizing experience caused by these extraordinary so-called airlines\"\n\n# Tokenize and encode the tweet\nencoded_tweet = tokenizer(tweet, padding=True, truncation=True, return_tensors=\"pt\")\n\n# Make predictions\nwith torch.no_grad():\n    model.eval()\n    outputs = model(**encoded_tweet)\n    predictions = torch.argmax(outputs.logits, dim=1)\n\n# Map predictions to labels\nsentiment_labels = ['negative', 'neutral', 'positive']\npredicted_sentiment = sentiment_labels[predictions.item()]\n\nprint(\"The sentiment of the tweet is: \",{predicted_sentiment})\n","metadata":{"execution":{"iopub.status.busy":"2023-10-29T16:27:10.814994Z","iopub.execute_input":"2023-10-29T16:27:10.816038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer, TrainingArguments, Trainer\nfrom datasets import load_dataset\n\n# Load the pre-trained RoBERTa model and tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/input/roberta-base\")\nmodel = RobertaForSequenceClassification.from_pretrained(\"/kaggle/input/roberta-base\")\n\n# Load your dataset (assuming you have it in a format compatible with the 'datasets' library)\ndataset = load_dataset('/kaggle/input/twitter-airline-sentiment')\n\n# Tokenize and preprocess your dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"steps\",\n    save_steps=500,\n    eval_steps=500,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    evaluation_strategy=\"steps\",\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    remove_unused_columns=False,\n    push_to_hub=False,\n)\n\n# Define a data collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\n# Start fine-tuning\ntrainer.train()\n\n# Save the model\ntrainer.save_model('./your_fine_tuned_model')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}